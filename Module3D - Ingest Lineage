import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

CatalogDataSource_dsource1 = spark.sql("select * from `personalbanking_raw`.`raw_accounts`")


# Set S3 output location for your data
s3_output_path = "s3://<account-id>-us-east-1-datagov-personalbanking-curated/personalbanking_curated/accounts2/" # Inserir o endere√ßo do bucket s3://<account-id>-us-east-1-datagov-personalbanking-curated/personalbanking_curated/accounts2/

# Create the table first to ensure proper format specification
spark.sql(f"""
CREATE TABLE IF NOT EXISTS personalbanking_curated.accounts2
USING parquet
LOCATION '{s3_output_path}'
AS SELECT * FROM `personalbanking_raw`.`raw_accounts` WHERE 1=0
""")

# Then write the data
CatalogDataSource_dsource1.write \
    .mode("overwrite") \
    .format("parquet") \
    .option("path", s3_output_path) \
    .insertInto("personalbanking_curated.accounts2")

job.commit()
